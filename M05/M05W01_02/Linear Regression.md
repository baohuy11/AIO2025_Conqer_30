# Linear Regression

## **1. Linear Regression**

### **1.1. Introduction**

Linear regression lÃ  má»™t thuáº­t toÃ¡n há»c cÃ³ giÃ¡m sÃ¡t trong machine learning, nÆ¡i thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c biáº¿n Ä‘á»™c láº­p vÃ  biáº¿n phá»¥ thuá»™c. NÃ³ giáº£ Ä‘á»‹nh ráº±ng má»‘i quan há»‡ cá»§a biáº¿n Ä‘á»™c láº­p vÃ  biáº¿n phá»¥ thuá»™c lÃ  tuyáº¿n tÃ­nh. Cá»¥ thá»ƒ, phÆ°Æ¡ng phÃ¡p nÃ y sáº½ há»c tá»« táº­p dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gáº¯n nhÃ£n vÃ  Ã¡nh xáº¡ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u thÃ nh má»™t hÃ m tuyáº¿n tÃ­nh Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ dá»± Ä‘oÃ¡n trÃªn táº­p dá»¯ liá»‡u má»›i.

PhÆ°Æ¡ng trÃ¬nh tá»•ng quÃ¡t cá»§a linear regression:
$y \approx \hat{y} = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n$

trong Ä‘Ã³:

- **$y$**: giÃ¡ trá»‹ thá»±c cá»§a Ä‘áº§u ra (ground truth)

- **$\hat y$**: biáº¿n phá»¥ thuá»™c (giÃ¡ trá»‹ Ä‘áº§u ra dá»± Ä‘oÃ¡n - predict output)

- **$x_1, x_2, \dots, x_n$**: cÃ¡c biáº¿n Ä‘á»™c láº­p

- **$w_0, w_1, \dots, w_n$**: cÃ¡c há»‡ sá»‘ cáº§n tÃ¬m

Má»¥c tiÃªu cá»§a Linear regression lÃ  Æ°á»›c tÃ­nh cÃ¡c há»‡ sá»‘ $w$ Ä‘á»ƒ giáº£m thiá»ƒu sai sá»‘ dá»± Ä‘oÃ¡n cho biáº¿n phá»¥ thuá»™c - $\hat y$ dá»±a trÃªn táº­p há»£p cÃ¡c biáº¿n Ä‘á»™c láº­p - $x$. Äá»ƒ thá»±c hiá»‡n Ä‘iá»u nÃ y, má»™t cÃ¡ch hiá»ƒu Ä‘Æ¡n giáº£n lÃ  cá»‘ gáº¯ng tÃ¬m má»™t phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh Ä‘á»ƒ cho $\hat y$ xáº¥p xá»‰ vá»›i giÃ¡ trá»‹ $y$. Máº·c dÃ¹, $y$ vÃ  $\hat y$ lÃ  hai giÃ¡ trá»‹ khÃ¡c nhau do sai sá»‘ cá»§a mÃ´ hÃ¬nh; tuy nhiÃªn chÃºng ta mong muá»‘n sá»± khÃ¡c nhau giá»¯a hai giÃ¡ trá»‹ lÃ  nhá» nháº¥t.

---

### **1.2. HÃ m máº¥t mÃ¡t cho linear regression**

NhÆ° Ä‘Ã£ tháº£o luáº­n trÆ°á»›c Ä‘Ã³, viá»‡c tÃ¬m ra má»™t phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh cho cÃ¡c trÆ°á»ng há»£p thá»±c táº¿ lÃ  khÃ´ng dá»… dÃ ng vÃ¬ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n sáº½ phÃ¡t sinh lá»—i. Nhá»¯ng lá»—i nÃ y cáº§n Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘á»ƒ giáº£m thiá»ƒu chÃºng. Sá»± khÃ¡c biá»‡t giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n $\hat y$ vÃ  giÃ¡ trá»‹ thá»±c $y$ Ä‘Æ°á»£c gá»i lÃ  hÃ m máº¥t mÃ¡t.

HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c hÃ¬nh thÃ nh tá»« viá»‡c láº¥y giÃ¡ trá»‹ dá»± Ä‘oÃ¡n $\hat y$ vÃ  so sÃ¡nh nÃ³ vá»›i giÃ¡ trá»‹ má»¥c tiÃªu (target) $y$ báº±ng phÃ©p tÃ­nh $(\hat y - y)$ Ä‘á»ƒ Ä‘o lÆ°á»ng khoáº£ng cÃ¡ch giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ má»¥c tiÃªu.

â¡ï¸-> ChÃºng ta muá»‘n tÃ¬m cÃ¡c giÃ¡ trá»‹ $w$ Ä‘á»ƒ sai sá»‘ $(\hat y-y)$ lÃ  nhá» nháº¥t.

á» Ä‘Ã¢y, phÃ©p tÃ­nh $(\hat y-y)$ Ä‘Ã´i khi lÃ  sá»‘ Ã¢m nÃªn ta cÃ³ thá»ƒ tÃ­nh toÃ¡n báº±ng cÃ¡ch láº¥y giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i $|\hat y-y|$ hoáº·c láº¥y bÃ¬nh phÆ°Æ¡ng $(\hat y-y)^2$. Trong linear regression, cÃ¡ch láº¥y giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i Ã­t Ä‘Æ°á»£c sá»­ dá»¥ng hÆ¡n vÃ¬ hÃ m nÃ y khÃ´ng kháº£ vi táº¡i má»i Ä‘iá»ƒm, sáº½ khÃ´ng thuáº­n tiá»‡n cho viá»‡c tá»‘i Æ°u sau nÃ y.

ChÃ­nh vÃ¬ váº­y, **MSE - Mean Squared Error** Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n giÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c sai sá»‘ bÃ¬nh phÆ°Æ¡ng giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c.

HÃ m MSE cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh nhÆ° sau:

$$J = \frac{1}{n}\sum_{i=1}^{n}(\hat y_i - y_i)^2$$

BÃ i toÃ¡n lÃºc nÃ y chuyá»ƒn thÃ nh tÃ¬m cÃ¡c giÃ¡ trá»‹ $w$ Ä‘á»ƒ giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t $J$ lÃ  nhá» nháº¥t.

---

### **1.3. TÃ³m táº¯t**

- Linear regression mong muá»‘n cÃ³ má»™t Ä‘Æ°á»ng tháº³ng phÃ¹ há»£p cho bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n vá»›i phÆ°Æ¡ng trÃ¬nh: $y = w_0 + w_ix_i$

- **Tham sá»‘**: $w$

- TÃ¹y vÃ o cÃ¡ch chá»n cÃ¡c tham sá»‘ $w$ ta sáº½ cÃ³ Ä‘Æ°á»£c cÃ¡c Ä‘Æ°á»ng tháº³ng khÃ¡c nhau.

- **Má»¥c tiÃªu**: Cáº§n tÃ¬m $w$ sao cho Ä‘Æ°á»ng tháº³ng phÃ¹ há»£p nháº¥t vá»›i bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n.

- Äá»ƒ Ä‘o má»©c Ä‘á»™ lá»±a chá»n $w$ phÃ¹ há»£p, ta cÃ³ **hÃ m máº¥t mÃ¡t (Loss Function)**.

- **HÃ m máº¥t mÃ¡t**: HÃ m nÃ y Ä‘o lÆ°á»ng sai sá»‘ giá»¯a cÃ¡c dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh vÃ  giÃ¡ trá»‹ thá»±c táº¿.

  $$J = \frac{1}{n}\sum_{i=1}^{n}(\hat y_i - y_i)^2$$
- **Má»¥c tiÃªu cuá»‘i cÃ¹ng**: TÃ¬m cÃ¡c giÃ¡ trá»‹ $w$ Ä‘á»ƒ hÃ m máº¥t mÃ¡t $J$ lÃ  nhá» nháº¥t.

  $$w = \text{minimize}_w J(w)$$

---

## **2. Gradient Descent**

### **2.1. Gradient descent lÃ  gÃ¬?**

Gradient descent (GD) lÃ  má»™t ká»¹ thuáº­t tá»‘i Æ°u phá»• biáº¿n, thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vÃ  máº¡ng nÆ¡-ron. NÃ³ giÃºp huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh báº±ng cÃ¡ch giáº£m thiá»ƒu sai sá»‘ giá»¯a káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ  káº¿t quáº£ thá»±c táº¿.

Trong linear regression, ká»¹ thuáº­t nÃ y giÃºp chÃºng ta Ä‘i tÃ¬m Ä‘iá»ƒm tá»‘i Æ°u trÃªn hÃ m máº¥t mÃ¡t, nÆ¡i mÃ  sai sá»‘ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c giáº£m thiá»ƒu tá»‘i Ä‘a. NhÆ° váº­y, cÃ³ thá»ƒ nÃ³i hÃ m máº¥t mÃ¡t Ä‘Ã³ng vai trÃ² nhÆ° má»™t thÆ°á»›c Ä‘o; cÃ²n Gradient descent lÃ  cÃ´ng cá»¥ Ä‘á»ƒ tÃ¬m ra tham sá»‘ tá»‘i Æ°u nháº¥t cho hÃ m máº¥t mÃ¡t.

---

### **2.2. CÃ¡ch hoáº¡t Ä‘á»™ng**

Gradient descent hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch báº¯t Ä‘áº§u vá»›i cÃ¡c tham sá»‘ mÃ´ hÃ¬nh ngáº«u nhiÃªn vÃ  liÃªn tá»¥c Ä‘iá»u chá»‰nh chÃºng Ä‘á»ƒ lÃ m giáº£m sá»± khÃ¡c biá»‡t giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿.

Vá»›i GD, viá»‡c cáº§n lÃ m lÃ  tiáº¿p tá»¥c thay Ä‘á»•i cÃ¡c tham sá»‘ $w$ má»—i láº§n má»™t chÃºt Ä‘á»ƒ cá»‘ gáº¯ng giáº£m giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t $J(w)$ cho Ä‘áº¿n khi $J$ á»•n Ä‘á»‹nh hoáº·c gáº§n má»©c tá»‘i thiá»ƒu.

**CÃ´ng thá»©c cáº­p nháº­t:**

$$w = w - \alpha \cdot \nabla J(w)$$

Trong Ä‘Ã³:

- **$w$**: lÃ  trá»ng sá»‘ tá»‘i Æ°u cáº§n tÃ¬m

- **$\alpha$ (alpha)**: learning rate - lÃ  tá»‘c Ä‘á»™ há»c, xÃ¡c Ä‘á»‹nh Ä‘á»™ lá»›n cá»§a má»—i bÆ°á»›c cáº­p nháº­t.

- **$\nabla J(w)$**: Ä‘áº¡o hÃ m riÃªng cá»§a hÃ m máº¥t mÃ¡t theo tá»«ng tham sá»‘.

Learning rate thÆ°á»ng lÃ  sá»‘ nhá» giá»¯a 0 vÃ  1, vá» cÆ¡ báº£n $\alpha$ kiá»ƒm soÃ¡t Ä‘á»™ lá»›n bÆ°á»›c di chuyá»ƒn cá»§a trá»ng sá»‘. Äáº¡o hÃ m giÃºp xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c hÆ°á»›ng di chuyá»ƒn cá»§a trá»ng sá»‘; cÃ¡c trá»ng sá»‘ sáº½ di chuyá»ƒn ngÆ°á»£c vá»›i hÆ°á»›ng cá»§a Ä‘áº¡o hÃ m Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm tá»‘i Æ°u.

GD sáº½ láº·p láº¡i cho Ä‘áº¿n khi há»™i tá»¥ - Ä‘áº¡t Ä‘áº¿n Ä‘iá»ƒm local minimum (nÆ¡i cÃ¡c tham sá»‘ $w$ khÃ´ng cÃ²n thay Ä‘á»•i nhiá»u vá»›i má»—i bÆ°á»›c di chuyá»ƒn).

á» Ä‘Ã¢y Ä‘iá»ƒm local minimum cÃ³ thá»ƒ lÃ  global minimum, tuy nhiÃªn viá»‡c tÃ¬m ra Ä‘iá»ƒm global minimum cá»§a cÃ¡c hÃ m máº¥t mÃ¡t lÃ  ráº¥t phá»©c táº¡p tháº­m chÃ­ lÃ  báº¥t kháº£ thi. Thay vÃ o Ä‘Ã³, ngÆ°á»i ta thÆ°á»ng cá»‘ gáº¯ng tÃ¬m ra cÃ¡c Ä‘iá»ƒm local minimum vÃ  coi Ä‘Ã³ lÃ  nghiá»‡m cá»§a bÃ i toÃ¡n. Do váº­y, tÃ¹y vÃ o Ä‘iá»ƒm khá»Ÿi táº¡o tham sá»‘ $w$ á»Ÿ Ä‘Ã¢u sáº½ cÃ³ thá»ƒ cÃ³ local minimum khÃ¡c nhau.

---

### **2.3. CÃ¡c loáº¡i Gradient descent**

- **Batch Gradient Descent**: Sá»­ dá»¥ng toÃ n bá»™ táº­p dá»¯ liá»‡u Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ trong má»—i bÆ°á»›c láº·p. PhÆ°Æ¡ng phÃ¡p nÃ y cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t vÃ¬ cÃ¡c tham sá»‘ khi cáº­p nháº­t Ä‘Æ°á»£c xem xÃ©t trÃªn toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n. Tuy nhiÃªn nÃ³ tá»‘n nhiá»u chi phÃ­ tÃ­nh toÃ¡n Ä‘á»‘i vá»›i cÃ¡c bá»™ dá»¯ liá»‡u lá»›n.

- **Stochastic Gradient Descent (SGD)**: Chá»‰ sá»­ dá»¥ng má»™t máº«u dá»¯ liá»‡u huáº¥n luyá»‡n ngáº«u nhiÃªn Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘. Tá»‘c Ä‘á»™ há»™i tá»¥ nhanh, nhÆ°ng Ä‘á»™ chÃ­nh xÃ¡c tháº¥p hÆ¡n so vá»›i Batch Gradient Descent.

- **Mini-Batch Gradient Descent**: CÃ¡c tham sá»‘ Ä‘Æ°á»£c cáº­p nháº­t theo tá»«ng nhÃ³m nhá» (batch). ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p káº¿t há»£p sá»± á»•n Ä‘á»‹nh cá»§a Batch Gradient Descent vÃ  tá»‘c Ä‘á»™ cá»§a SGD. Tuy nhiÃªn, dá»¯ liá»‡u pháº£i cáº§n thá»i gian Ä‘á»ƒ lá»±a chá»n kÃ­ch thÆ°á»›c batch phÃ¹ há»£p; Ä‘iá»u nÃ y cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng kháº£ nÄƒng há»™i tá»¥ vÃ  hiá»‡u suáº¥t.

NgoÃ i ra Ä‘á»ƒ cáº£i tiáº¿n GD, má»™t sá»‘ biáº¿n thá»ƒ Ä‘Æ°á»£c giá»›i thiá»‡u nhÆ° **Momentum**, **Nesterov Accelerated Gradient (NAG)**, **Adagrad**, **RMSProp**, **Adam**.

---

### **2.4. Tá»•ng káº¿t**

Gradient Descent lÃ  thuáº­t toÃ¡n tá»‘i Æ°u cá»‘t lÃµi trong há»c mÃ¡y vÃ  Ä‘áº·c biá»‡t quan trá»ng trong deep learning hiá»‡n nay. NÃ³ giÃºp mÃ´ hÃ¬nh há»c báº±ng cÃ¡ch liÃªn tá»¥c Ä‘iá»u chá»‰nh tham sá»‘ Ä‘á»ƒ giáº£m sai sá»‘ dá»± Ä‘oÃ¡n.

- **Æ¯u Ä‘iá»ƒm**:

  - ÄÆ¡n giáº£n, dá»… triá»ƒn khai.

  - Hoáº¡t Ä‘á»™ng tá»‘t vá»›i dá»¯ liá»‡u lá»›n.

  - LÃ  ná»n táº£ng cho cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u hiá»‡n Ä‘áº¡i nhÆ° Adam, RMSProp hay Adagrad.

- **NhÆ°á»£c Ä‘iá»ƒm**:

  - Dá»… máº¯c káº¹t táº¡i local minimum.

  - Phá»¥ thuá»™c máº¡nh vÃ o tá»‘c Ä‘á»™ há»c (learning rate).

  - CÃ³ thá»ƒ há»™i tá»¥ cháº­m.

Trong thá»±c táº¿, cÃ¡c biáº¿n thá»ƒ nhÆ° **Mini-Batch GD** vÃ  **Adam** Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n nhá» cÃ¢n báº±ng tá»‘t giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh.

---

### **2.5. Code implementation**

```python
import numpy as np

def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:

	m, n = X.shape

	theta = np.zeros(n)

    for _ in range(iterations):

        predictions = X @ theta

        errors = predictions - y

        gradient = (1 / m) * (X.T @ errors)

        theta -= alpha * gradient

	return np.round(theta, 4)
```

---

## **3. Vetorizer Linear Regression**

ChÃºng ta sáº½ tÃ¬m hiá»ƒu vá» viá»‡c Ã¡p dá»¥ng vectorized trong linear regression, viá»‡c dÃ¹ng vectorized sáº½ giÃºp chÃºng ta tiáº¿t kiá»‡m thá»i gian tÃ­nh toÃ¡n dá»±a vÃ o viá»‡c tÃ­nh toÃ¡n cá»§a ma tráº­n nhanh hÆ¡n nhiá»u láº§n so vá»›i viá»‡c tÃ­nh toÃ¡n tá»«ng láº§n láº·p 1 vÃ  cáº­p nháº­t tham sá»‘.

Äá»ƒ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c cÃ¡ch dÃ¹ng vectorized, báº¡n sáº½ cáº§n cÃ³ kiáº¿n thá»©c cÆ¡ báº£n vá» ma tráº­n (chiá»u vÃ  cÃ¡ch nhÃ¢n chia cá»™ng trá»«, dot product).

### **CÃ´ng thá»©c vÃ  cÃ¡c bÆ°á»›c trong há»c mÃ¡y linear regression:**

1.  **Chá»n 1 dá»¯ liá»‡u trong data**

2.  **TÃ­nh Ä‘áº§u ra $\hat{Y}$**

    $\hat{Y} = w_1x_1 + w_2x_2 + \dots + w_ix_i + b$
    
    Trong Ä‘Ã³ $x_1, x_2, \dots, x_i$ tÆ°Æ¡ng á»©ng vá»›i giÃ¡ trá»‹ cá»™t tá»« 1, 2, ..., i.

    $w_1, w_2, \dots, w_i$ vÃ  $b$ lÃ  cÃ¡c Ä‘á»‘i sá»‘ khá»Ÿi táº¡o ban Ä‘áº§u ngáº«u nhiÃªn.

3.  **TÃ­nh loss**

    $L = (\hat{Y} - y)^2$

4.  **TÃ­nh Ä‘áº¡o hÃ m theo w vÃ  b**

$$ 
\frac{\partial L}{\partial w_1} = 2x_1(\hat{y} - y) 
$$

$$ 
\frac{\partial L}{\partial w_2} = 2x_2(\hat{y} - y) 
$$

$$
\frac{\partial L}{\partial w_3} = 2x_3(\hat{y} - y) 
$$

$$ 
\frac{\partial L}{\partial b} = 2(\hat{y} - y) 
$$

5.  **Cáº­p nháº­t tham sá»‘ $w_1, w_2, \dots, w_i$ vÃ  b**

### **Dá»¯ liá»‡u máº«u Ä‘á»ƒ tÃ­nh toÃ¡n:**

| Features | | | Label |
| :--- | :--- | :--- | :--- |
| **TV** | **Radio** | **Newspaper** | **Sales** |
| 230.1 | 37.8 | 69.2 | 22.1 |
| 44.5 | 39.3 | 45.1 | 10.4 |

Tá»« dá»¯ liá»‡u máº«u, ta cÃ³ cá»™t TV tÆ°Æ¡ng á»©ng $x_1$, Radio tÆ°Æ¡ng á»©ng $x_2$ vÃ  Newspaper tÆ°Æ¡ng á»©ng $x_3$, cá»™t Sales tÆ°Æ¡ng á»©ng $y$ (giÃ¡ trá»‹ thá»±c táº¿).

---

### VÃ­ dá»¥ tÃ­nh toÃ¡n vá»›i má»™t dÃ²ng dá»¯ liá»‡u:

**B1. Khá»Ÿi táº¡o tham sá»‘ $w_1, w_2, w_3, b = 0.01$, n (learning rate) = 0.0001**

Chá»n dÃ²ng dá»¯ liá»‡u Ä‘áº§u tiÃªn ta cÃ³ $x_1 = 230.1$, $x_2 = 37.8$, $x_3 = 69.2$, vÃ  $y = 22.1$.

**B2. Tiáº¿p theo, Ã¡p dá»¥ng cÃ´ng thá»©c ta cÃ³:**

$$
\hat{Y} = 0.01*230.1 + 0.01*37.8 + 0.01*69.2 + 0.01 = 3.38
$$

**B3. TÃ­nh Loss = $(\hat{Y}-y)^2 = (3.38-22.1)^2 \approx 350.43$**

**B4. TÃ­nh Ä‘áº¡o hÃ m theo w vÃ  b**

Ta cÃ³ $\hat{Y}-y = 3.38 - 22.1 = -18.72$

- Äáº¡o hÃ m táº¡i $w_1$: $2 * 230.1 * (-18.72) = -8614.944$

- Äáº¡o hÃ m táº¡i $w_2$: $2 * 37.8 * (-18.72) = -1415.232$

- Äáº¡o hÃ m táº¡i $w_3$: $2 * 69.2 * (-18.72) = -2590.848$

- Äáº¡o hÃ m táº¡i $b$: $2 * (-18.72) = -37.44$

**B5. Cáº­p nháº­t $w_1, w_2, w_3$ vÃ  b**

$w_1 = 0.01 - 0.0001 * (-8614.944) = 0.8714944$

$w_2 = 0.01 - 0.0001 * (-1415.232) = 0.1515232$

$w_3 = 0.01 - 0.0001 * (-2590.848) = 0.2690848$

$b = 0.01 - 0.0001 * (-37.44) = 0.013744$

Ta sáº½ tiáº¿p tá»¥c dÃ¹ng dÃ²ng thá»© 2 cá»§a data vÃ  tÃ­nh toÃ¡n theo 5 bÆ°á»›c trÃªn Ä‘á»ƒ thu Ä‘Æ°á»£c giÃ¡ trá»‹ tham sá»‘ $w_1, w_2, w_3, b$ tá»‘i Æ°u nháº¥t.

**ÄÃ¢y lÃ  káº¿t quáº£ sau khi cháº¡y dÃ²ng thá»© 2:**
`w1 = 0.46`, `w2 = -0.21`, `w3 = -0.15`, `b = 0.0045`

NhÆ° cÃ¡c báº¡n tháº¥y á»Ÿ trÃªn cÃ¡c tham sá»‘ sáº½ Ä‘Æ°á»£c cáº­p nháº­t sau má»—i láº§n huáº¥n luyá»‡n 1 dÃ²ng data, viá»‡c nÃ y sáº½ Ä‘Æ¡n giáº£n vÃ  Ã­t tá»‘n thá»i gian so vá»›i dá»¯ liá»‡u nhá», vá»›i dá»¯ liá»‡u lá»›n viá»‡c nÃ y sáº½ tá»‘n nhiá»u thá»i gian Ä‘á»ƒ huáº¥n luyá»‡n. Do Ä‘Ã³ viá»‡c Ã¡p dá»¥ng vectorized sáº½ giÃºp tiáº¿t kiá»‡m thá»i gian tÃ­nh toÃ¡n.

Vá»›i dá»¯ liá»‡u nhÆ° sau, ta sáº½ biáº¿n Ä‘á»•i cÃ´ng thá»©c sang dáº¡ng vector nhÆ° sau á»Ÿ cÃ¡c bÆ°á»›c b1, b2, b4, b5:

<div style="border: 2px solid #666; padding: 15px; border-radius: 8px;">

1.  Pick a sample $(\mathbf{x}, y)$ from training data

2.  Compute output $\hat{y}$

$$
\hat{y} = \boldsymbol{\theta}^T \mathbf{x} = \mathbf{x}^T \boldsymbol{\theta}
$$

3.  Compute loss

$$
L = (\hat{y} - y)^2
$$

4.  Compute gradient

$$
\nabla_{\boldsymbol{\theta}}L = 2\mathbf{x}(\hat{y} - y)
$$

5.  Update parameters

$$
\boldsymbol{\theta} = \boldsymbol{\theta} - \eta \nabla_{\boldsymbol{\theta}}L
$$

where $\eta$ is the learning rate.

</div>

Khá»Ÿi táº¡o tham sá»‘ $w_1, w_2, w_3, b = 0.01, n = 0.0001$

### B1: Ta táº¡o 1 ma tráº­n X tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhÆ° sau:

Matrix X:

$$
X = 
\begin{pmatrix}
1 & 1 \\
230.1 & 44.5 \\
37.8 & 39.3 \\
69.2 & 45.1
\end{pmatrix}
$$

Táº¡o ma tráº­n Y tÆ°Æ¡ng á»©ng vá»›i cá»™t sales:

$$
Y = 
\begin{pmatrix}
22.1 \\
10.4
\end{pmatrix}
$$

Táº¡o ma tráº­n $\theta$:

$$
\theta =
\begin{pmatrix}
0.01 \\
0.01 \\
0.01 \\
0.01
\end{pmatrix}
$$

### B2: TÃ­nh $\hat{Y}$, ta cÃ³

$$\hat{Y} = X^T \cdot \theta = 
\begin{pmatrix}
1 & 230.1 & 37.8 & 69.2 \\
1 & 44.5 & 39.2 & 45.1
\end{pmatrix}
\cdot
\begin{pmatrix}
0.01 \\
0.01 \\
0.01 \\
0.01
\end{pmatrix} =
\begin{pmatrix}
3.38 \\
1.30
\end{pmatrix}
$$

Ta thu Ä‘Æ°á»£c káº¿t quáº£ $\hat{Y}$ sau:

$$
\hat{Y} =
\begin{pmatrix}
3.381 \\
1.298
\end{pmatrix}
$$

### B3. TÃ­nh loss

CÃ´ng thá»©c MSE dáº¡ng ma tráº­n:

$$
L = \frac{1}{m} (\hat{Y} - Y)^T (\hat{Y} - Y)
$$

Trong Ä‘Ã³ $m$ lÃ  sá»‘ lÆ°á»£ng máº«u dá»¯ liá»‡u (á»Ÿ Ä‘Ã¢y $m=2$).


1.  **TÃ­nh vector sai sá»‘ ($E = \hat{Y} - Y$):**

$$
E = 
\begin{pmatrix} 3.381 \\ 1.299 \end{pmatrix} - 
\begin{pmatrix} 22.1 \\ 10.4 \end{pmatrix} = 
\begin{pmatrix} -18.719 \\ -9.101 \end{pmatrix}
$$

2.  **TÃ­nh Loss (MSE):**

$$
L = \frac{1}{2} \left( (-18.719)^2 + (-9.101)^2 \right) = \frac{1}{2} (350.40 + 82.83) = 216.615
$$

GiÃ¡ trá»‹ loss lÃ  **216.615**.

### B4: Äáº¡o hÃ m theo w vÃ  b:

CÃ´ng thá»©c tÃ­nh gradient cho MSE:
$$\nabla L = \frac{\partial L}{\partial \theta} = \frac{2}{m} X (\hat{Y} - Y)$$

1.  **Ãp dá»¥ng cÃ´ng thá»©c:**

    VÃ¬ $m=2$, há»‡ sá»‘ $\frac{2}{m} = 1$. Ta cÃ³:
    
$$
\nabla L = 1 \cdot X \cdot E = \begin{pmatrix} 1 & 1 \\ 230.1 & 44.5 \\ 37.8 & 39.3 \\ 69.2 & 45.1 \end{pmatrix} \cdot \begin{pmatrix} -18.719 \\ -9.101 \end{pmatrix}
$$

2.  **Thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n:**

$$
\nabla L =
\begin{pmatrix}
1(-18.719) + 1(-9.101) \\
230.1(-18.719) + 44.5(-9.101) \\
37.8(-18.719) + 39.3(-9.101) \\
69.2(-18.719) + 45.1(-9.101)
\end{pmatrix} =
\begin{pmatrix}
-27.82 \\
-4712.24 \\
-1065.25 \\
-1705.82
\end{pmatrix}
$$

### B5: Cáº­p nháº­t tham sá»‘

Sá»­ dá»¥ng vector gradient vá»«a tÃ­nh Ä‘Æ°á»£c Ä‘á»ƒ cáº­p nháº­t $\theta$:

$$\theta_{new} = \theta_{old} - \eta \nabla L$$

$$
\begin{pmatrix}
0.01 \\
0.01 \\
0.01 \\
0.01
\end{pmatrix} - 0.0001 \cdot
\begin{pmatrix}
-27.82 \\
-4712.24 \\
-1065.25 \\
-1705.82
\end{pmatrix} =
\begin{pmatrix}
0.01 - (-0.00278) \\
0.01 - (-0.47122) \\
0.01 - (-0.10652) \\
0.01 - (-0.17058)
\end{pmatrix} =
\begin{pmatrix}
0.01278 \\
0.48122 \\
0.11652 \\
0.18058
\end{pmatrix}
$$

$$
\begin{pmatrix}
w_1 \\
w_2 \\
w_3 \\
b
\end{pmatrix} =
\begin{pmatrix}
0.01278 \\
0.48122 \\
0.11652 \\
0.18058
\end{pmatrix}
$$

Do ta cÃ³ 2 data nÃªn sáº½ cáº§n pháº£i cháº¡y 2 láº§n láº·p láº¡i b1 vÃ  giá»¯ nguyÃªn tham sá»‘ má»›i nháº¥t, ta thu Ä‘Æ°á»£c:

$w_1 = 0.01278, w_2 = 0.48122, w_3 = 0.11652, b = 0.18058$

NhÆ° váº­y viá»‡c Ã¡p dá»¥ng vectorized vÃ  sá»­ dá»¥ng ma tráº­n sáº½ tÃ­nh toÃ¡n toÃ n bá»™ data trong 1 láº§n duy nháº¥t mÃ  khÃ´ng cáº§n pháº£i tÃ­nh toÃ¡n tá»«ng dÃ²ng 1. Viá»‡c dÃ¹ng vectorized giÃºp chÃºng ta tÃ­nh toÃ¡n táº¥t cáº£ data trong cÃ¹ng 1 láº§n duy nháº¥t mÃ  khÃ´ng pháº£i Ä‘i tá»«ng dÃ²ng Ä‘á»ƒ tÃ­nh toÃ¡n, tá»« Ä‘Ã³ cáº£i thiá»‡n Ä‘Æ°á»£c thá»i gian tÃ­nh toÃ¡n lÃªn ráº¥t nhiá»u.

---

## **4. Normal Equation**

### **4.1. Normal Equations lÃ  gÃ¬?**

**Normal Equation** lÃ  má»™t phÆ°Æ¡ng phÃ¡p giáº£i tÃ­ch (analytical solution) Ä‘á»ƒ tÃ¬m ra bá»™ tham sá»‘ tá»‘i Æ°u cho mÃ´ hÃ¬nh Linear Regression. Thay vÃ¬ pháº£i huáº¥n luyá»‡n mÃ´ hÃ¬nh qua nhiá»u vÃ²ng láº·p nhÆ° Gradient Descent, Normal Equation cho phÃ©p chÃºng ta tÃ­nh toÃ¡n trá»±c tiáº¿p giÃ¡ trá»‹ tham sá»‘ tá»‘i Æ°u chá»‰ báº±ng má»™t cÃ´ng thá»©c duy nháº¥t.

**Ã tÆ°á»Ÿng cá»‘t lÃµi:** Thay vÃ¬ "dÃ² dáº«m" tÃ¬m kiáº¿m nghiá»‡m tá»‘i Æ°u nhÆ° Gradient Descent, Normal Equation giáº£i trá»±c tiáº¿p phÆ°Æ¡ng trÃ¬nh Ä‘áº¡o hÃ m báº±ng 0 Ä‘á»ƒ tÃ¬m Ä‘iá»ƒm cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t (loss function).

---


### **4.2. CÃ´ng thá»©c cá»§a Normal Equation cá»§a linear regression:**

$$
\boldsymbol{\theta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}
$$

Trong Ä‘Ã³:

* $\boldsymbol{\theta}$ (theta): Vector chá»©a cÃ¡c tham sá»‘ cáº§n tÃ¬m cá»§a mÃ´ hÃ¬nh

* $\mathbf{X}$: Ma tráº­n dá»¯ liá»‡u Ä‘áº§u vÃ o (má»—i hÃ ng lÃ  má»™t máº«u dá»¯ liá»‡u)

* $\mathbf{X}^T$: Ma tráº­n chuyá»ƒn vá»‹ cá»§a X

* $\mathbf{y}$: Vector chá»©a giÃ¡ trá»‹ thá»±c táº¿ (target values)

* $(\mathbf{X}^T \mathbf{X})^{-1}$: Ma tráº­n nghá»‹ch Ä‘áº£o cá»§a $(\mathbf{X}^T \mathbf{X})$

---

### **4.3. CÃ¡ch hoáº¡t Ä‘á»™ng**

Minh há»a Linear Regression

![Minh há»a Linear Regression](../img/output_lr.png)

CÃ¡c Ä‘iá»ƒm xanh lÃ  dá»¯ liá»‡u, Ä‘Æ°á»ng Ä‘á» lÃ  Ä‘Æ°á»ng há»“i quy tÃ¬m Ä‘Æ°á»£c báº±ng Normal Equation

**BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u**

Giáº£ sá»­ chÃºng ta cÃ³ dá»¯ liá»‡u vá» diá»‡n tÃ­ch nhÃ  vÃ  giÃ¡ nhÃ :

Diá»‡n tÃ­ch (mÂ²) | GiÃ¡ (triá»‡u Ä‘á»“ng)
--- | ---
50 | 1500
80 | 2400
100 | 3000
120 | 3600

**BÆ°á»›c 2: XÃ¢y dá»±ng ma tráº­n X vÃ  vector y**

Ma tráº­n X cáº§n thÃªm cá»™t 1 á»Ÿ Ä‘áº§u (Ä‘á»ƒ tÃ­nh há»‡ sá»‘ cháº·n - intercept):

$$
\mathbf{X} = \begin{bmatrix}
1 & 50 \\
1 & 80 \\
1 & 100 \\
1 & 120
\end{bmatrix}
$$

Vector y:

$$
\mathbf{y} = \begin{bmatrix}
1500 \\
2400 \\
3000 \\
3600
\end{bmatrix}
$$


**BÆ°á»›c 3: Ãp dá»¥ng cÃ´ng thá»©c**

Thá»±c hiá»‡n cÃ¡c phÃ©p tÃ­nh ma tráº­n theo cÃ´ng thá»©c Normal Equation Ä‘á»ƒ tÃ¬m $\boldsymbol{\theta}$.

---

### **4.4. Code implementation**

```python
import numpy as np

# Data
X = np.array([[1, 50], [1, 80], [1, 100], [1, 120]])
y = np.array([[1500], [2400], [3000], [3600]])

# Apply Normal Equation
theta = np.linalg.inv(X.T @ X) @ X.T @ y

# Result: theta[0] = intercept, theta[1] = slope (gradient)
print("Optimal parameters:", theta) 
```

### Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm

| âœ… Æ¯u Ä‘iá»ƒm (Advantages) | âŒ NhÆ°á»£c Ä‘iá»ƒm (Disadvantages) |
| :---------------------------------------------- | :------------------------------------------------------------------ |
| KhÃ´ng cáº§n chá»n learning rate.                  | Cháº­m vá»›i dá»¯ liá»‡u lá»›n (khi sá»‘ features `n` > 10,000).                 |
| KhÃ´ng cáº§n láº·p nhiá»u vÃ²ng.                      | Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ  $O(n^3)$.                                  |
| TÃ­nh toÃ¡n má»™t láº§n ra káº¿t quáº£ chÃ­nh xÃ¡c.         | KhÃ´ng hoáº¡t Ä‘á»™ng náº¿u ma tráº­n $(\mathbf{X}^T \mathbf{X})$ khÃ´ng kháº£ nghá»‹ch. |
| ÄÆ¡n giáº£n, dá»… implement.                        | Tá»‘n bá»™ nhá»› Ä‘á»ƒ lÆ°u cÃ¡c ma tráº­n lá»›n.                                 |

---

### **4.5. So sÃ¡nh vá»›i Gradient Descent**

#### Khi nÃ o dÃ¹ng Normal Equation?

- Khi sá»‘ lÆ°á»£ng features nhá» (vÃ­ dá»¥: `n` â‰¤ 10,000).

- Khi cáº§n káº¿t quáº£ chÃ­nh xÃ¡c ngay láº­p tá»©c.

- Khi khÃ´ng muá»‘n pháº£i tinh chá»‰nh cÃ¡c siÃªu tham sá»‘ (hyperparameters) nhÆ° learning rate.

#### Khi nÃ o dÃ¹ng Gradient Descent?

- Khi sá»‘ lÆ°á»£ng features ráº¥t lá»›n (`n` > 10,000).

- Khi táº­p dá»¯ liá»‡u quÃ¡ lá»›n, khÃ´ng thá»ƒ táº£i háº¿t vÃ o RAM cÃ¹ng má»™t lÃºc.

- Khi cáº§n má»™t mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c online (cáº­p nháº­t khi cÃ³ dá»¯ liá»‡u má»›i).

---

### **4.6. Káº¿t luáº­n**

**Normal Equation** lÃ  má»™t cÃ´ng cá»¥ máº¡nh máº½ vÃ  thanh lá»‹ch trong Machine Learning, Ä‘áº·c biá»‡t phÃ¹ há»£p vá»›i cÃ¡c bÃ i toÃ¡n Linear Regression cÃ³ quy mÃ´ vá»«a pháº£i. Viá»‡c hiá»ƒu rÃµ cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a nÃ³ khÃ´ng chá»‰ giÃºp báº¡n giáº£i quyáº¿t váº¥n Ä‘á» hiá»‡u quáº£ mÃ  cÃ²n cá»§ng cá»‘ ná»n táº£ng toÃ¡n há»c trong ML.

**Äiá»ƒm quan trá»ng:** **Normal Equation** lÃ  má»™t vÃ­ dá»¥ tuyá»‡t vá»i cho tháº¥y khÃ´ng pháº£i lÃºc nÃ o cÅ©ng cáº§n "há»c" qua nhiá»u vÃ²ng láº·p â€“ Ä‘Ã´i khi chÃºng ta cÃ³ thá»ƒ "tÃ­nh toÃ¡n" trá»±c tiáº¿p Ä‘á»ƒ ra ngay cÃ¢u tráº£ lá»i tá»‘i Æ°u!

---

## **5. HÃ m Máº¥t MÃ¡t (Loss Function): "Kim Chá»‰ Nam" GiÃºp MÃ´ HÃ¬nh Há»c ChÃ­nh XÃ¡c**

### **5.1. Giá»›i Thiá»‡u: Táº¡i Sao MÃ´ HÃ¬nh Cáº§n Má»™t "Loss Function"?**

HÃ£y tÆ°á»Ÿng tÆ°á»£ng báº¡n Ä‘ang dáº¡y má»™t cá»— mÃ¡y dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn diá»‡n tÃ­ch. LÃ m sao nÃ³ biáº¿t dá»± Ä‘oÃ¡n cá»§a mÃ¬nh lÃ  "tá»‘t" hay "tá»‡"? ÄÃ³ lÃ  lÃºc HÃ m Máº¥t MÃ¡t (Loss Function) phÃ¡t huy tÃ¡c dá»¥ng.

HÃ m máº¥t mÃ¡t chÃ­nh lÃ  "kim chá»‰ nam" cho mÃ´ hÃ¬nh mÃ¡y há»c. NÃ³ Ä‘o lÆ°á»ng má»©c Ä‘á»™ sai lá»‡ch giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ thá»±c táº¿, tá»« Ä‘Ã³ chá»‰ cho mÃ´ hÃ¬nh biáº¿t cáº§n pháº£i "sá»­a sai" theo hÆ°á»›ng nÃ o Ä‘á»ƒ trá»Ÿ nÃªn chÃ­nh xÃ¡c hÆ¡n. ToÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh lÃ  má»™t hÃ nh trÃ¬nh liÃªn tá»¥c Ä‘iá»u chá»‰nh theo sá»± dáº«n dáº¯t cá»§a "kim chá»‰ nam" nÃ y Ä‘á»ƒ tÃ¬m ra Ä‘iá»ƒm cÃ³ sai sá»‘ tháº¥p nháº¥t.

---

### **5.2. MÃ´ HÃ¬nh "Há»c" NhÆ° Tháº¿ NÃ o?**

QuÃ¡ trÃ¬nh há»c cá»§a mÃ´ hÃ¬nh lÃ  má»™t vÃ²ng láº·p tá»‘i Æ°u hÃ³a gá»“m 5 bÆ°á»›c, Ä‘Æ°á»£c gá»i lÃ  **Gradient Descent**:

1.  **Láº¥y máº«u**: Chá»n má»™t cáº·p dá»¯ liá»‡u (vÃ­ dá»¥: diá»‡n tÃ­ch = 6.7, giÃ¡ = 9.1).

2.  **Dá»± Ä‘oÃ¡n**: DÃ¹ng cÃ¡c trá»ng sá»‘ $w$ vÃ  $b$ hiá»‡n táº¡i Ä‘á»ƒ tÃ­nh giÃ¡ dá»± Ä‘oÃ¡n: $\hat{y} = wx + b$.

3.  **TÃ­nh sai sá»‘ (Loss)**: So sÃ¡nh giÃ¡ dá»± Ä‘oÃ¡n vÃ  giÃ¡ tháº­t báº±ng hÃ m máº¥t mÃ¡t, vÃ­ dá»¥: 

$$
L = (\hat{y} - y)^2
$$

4.  **TÃ¬m hÆ°á»›ng sá»­a sai**: TÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t Ä‘á»ƒ biáº¿t cáº§n tÄƒng hay giáº£m $w$ vÃ  $b$.

5.  **Cáº­p nháº­t**: Äiá»u chá»‰nh $w$ vÃ  $b$ má»™t chÃºt theo hÆ°á»›ng ngÆ°á»£c láº¡i cá»§a Ä‘áº¡o hÃ m.

**VÃ­ dá»¥ nhanh:**

-   **Báº¯t Ä‘áº§u**: $w = -0.34$, $b = 0.04$.

-   **Dá»± Ä‘oÃ¡n (Láº§n 1)**: $\hat{y} = (-0.34) \times 6.7 + 0.04 = -2.238$ (ráº¥t tá»‡!).

-   **Sai sá»‘ (Láº§n 1)**: $L \approx 128.5$ (cá»±c ká»³ lá»›n).

-   **Sau khi cáº­p nháº­t (Láº§n 2)**: w vÃ  b Ä‘Æ°á»£c Ä‘iá»u chá»‰nh, giáº£ sá»­ thÃ nh $w = 1.179$, $b = 0.267$.

-   **Dá»± Ä‘oÃ¡n (Láº§n 2)**: $\hat{y} = 1.179 \times 6.7 + 0.267 \approx 8.168$ (gáº§n Ä‘Ãºng hÆ¡n nhiá»u!).

-   **Sai sá»‘ (Láº§n 2)**: $L \approx 0.868$ (giáº£m Ä‘Ã¡ng ká»ƒ!).

Láº·p láº¡i quÃ¡ trÃ¬nh nÃ y hÃ ng nghÃ¬n láº§n, mÃ´ hÃ¬nh sáº½ ngÃ y cÃ ng chÃ­nh xÃ¡c.

---

### **5.3. CÃ¡c HÃ m Máº¥t MÃ¡t Phá»• Biáº¿n**

#### **5.3.1. Mean Squared Error (MSE)**

-   **CÃ´ng thá»©c**: $L = (y - \hat{y})^2$.

-   **Äáº·c Ä‘iá»ƒm**: BÃ¬nh phÆ°Æ¡ng sai sá»‘, do Ä‘Ã³ "trá»«ng pháº¡t" cÃ¡c lá»—i lá»›n ráº¥t náº·ng ná». Ráº¥t nháº¡y cáº£m vá»›i cÃ¡c giÃ¡ trá»‹ ngoáº¡i lai (outliers).

-   **Æ¯u Ä‘iá»ƒm**: Bá» máº·t hÃ m máº¥t mÃ¡t lÃ¡ng má»‹n, giÃºp Gradient Descent há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh.

#### **5.3.2. Mean Absolute Error (MAE)**

-   **CÃ´ng thá»©c**: $L = |y - \hat{y}|$.

-   **Äáº·c Ä‘iá»ƒm**: Chá»‰ láº¥y trá»‹ tuyá»‡t Ä‘á»‘i cá»§a sai sá»‘, coi má»i lá»—i Ä‘á»u cÃ³ "giÃ¡" nhÆ° nhau. Bá»n vá»¯ng vÃ  Ã­t bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi outliers.

-   **NhÆ°á»£c Ä‘iá»ƒm**: CÃ³ má»™t "gÃ³c nhá»n" táº¡i Ä‘iá»ƒm sai sá»‘ báº±ng 0, cÃ³ thá»ƒ gÃ¢y khÃ³ khÄƒn cho viá»‡c tá»‘i Æ°u hÃ³a vÃ  lÃ m mÃ´ hÃ¬nh há»™i tá»¥ cháº­m.

#### **5.3.3. Huber Loss**

-   **Äáº·c Ä‘iá»ƒm**: LÃ  sá»± káº¿t há»£p hoÃ n háº£o giá»¯a MSE vÃ  MAE. Khi sai sá»‘ nhá» (nhá» hÆ¡n ngÆ°á»¡ng $\delta$), nÃ³ hoáº¡t Ä‘á»™ng nhÆ° MSE. Khi sai sá»‘ lá»›n, nÃ³ chuyá»ƒn sang hoáº¡t Ä‘á»™ng nhÆ° MAE.

-   **Æ¯u Ä‘iá»ƒm**: Vá»«a á»•n Ä‘á»‹nh, vá»«a bá»n vá»¯ng vá»›i outliers. ThÆ°á»ng lÃ  lá»±a chá»n máº·c Ä‘á»‹nh an toÃ n.

-   **CÃ´ng thá»©c**:

$$
L_{\delta}(y, \hat{y}) =
\begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \le \delta \\
\delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{if } |y - \hat{y}| > \delta
\end{cases}
$$

---

### **5.4. CÃ¡c Ká»¹ Thuáº­t Bá»• Sung**

Äá»ƒ hÃ m máº¥t mÃ¡t hoáº¡t Ä‘á»™ng hiá»‡u quáº£, chÃºng ta cáº§n:

-   **Chuáº©n HÃ³a Dá»¯ Liá»‡u (Data Normalization)**: Khi cÃ¡c Ä‘áº·c trÆ°ng cÃ³ thang Ä‘o quÃ¡ khÃ¡c nhau (vÃ­ dá»¥: diá»‡n tÃ­ch nhÃ  vÃ  sá»‘ phÃ²ng ngá»§), quÃ¡ trÃ¬nh há»c sáº½ máº¥t cÃ¢n báº±ng. Chuáº©n hÃ³a sáº½ Ä‘Æ°a táº¥t cáº£ vá» cÃ¹ng má»™t thang Ä‘o, giÃºp mÃ´ hÃ¬nh há»™i tá»¥ nhanh hÆ¡n.

-   **Regularization**: Khi mÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p vÃ  "há»c thuá»™c lÃ²ng" dá»¯ liá»‡u (overfitting), Regularization sáº½ thÃªm má»™t "thÃ nh pháº§n pháº¡t" vÃ o hÃ m máº¥t mÃ¡t Ä‘á»ƒ kiá»ƒm soÃ¡t Ä‘á»™ lá»›n cá»§a cÃ¡c trá»ng sá»‘, giÃºp mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.

    -   **L2 (Ridge)**: LÃ m cÃ¡c trá»ng sá»‘ nhá» láº¡i.

    -   **L1 (Lasso)**: CÃ³ thá»ƒ Ä‘áº©y cÃ¡c trá»ng sá»‘ khÃ´ng quan trá»ng vá» 0, giÃºp lá»±a chá»n Ä‘áº·c trÆ°ng.

---

### **5.5. Lá»i KhuyÃªn Thá»±c Tiá»…n: Chá»n GÃ¬ BÃ¢y Giá»?**

#### Báº£ng so sÃ¡nh cÃ¡c hÃ m máº¥t mÃ¡t

| TiÃªu chÃ­             | Mean Squared Error (MSE)        | Mean Absolute Error (MAE)       | Huber Loss                                                 |
| --------------------- | ------------------------------- | ------------------------------- | ---------------------------------------------------------- |
| **Äá»™ nháº¡y vá»›i Outliers** | Cao                             | Tháº¥p                            | Tháº¥p                                                       |
| **Tá»‘c Ä‘á»™ há»™i tá»¥** | Nhanh vÃ  á»•n Ä‘á»‹nh                 | CÃ³ thá»ƒ cháº­m vÃ  dao Ä‘á»™ng         | Nhanh vÃ  á»•n Ä‘á»‹nh                                           |
| **Khi nÃ o nÃªn dÃ¹ng?** | Dá»¯ liá»‡u sáº¡ch, khÃ´ng cÃ³ outliers. | Dá»¯ liá»‡u cÃ³ nhiá»u outliers cáº§n bá» qua. | Lá»±a chá»n máº·c Ä‘á»‹nh an toÃ n, Ä‘áº·c biá»‡t khi khÃ´ng cháº¯c vá» cháº¥t lÆ°á»£ng dá»¯ liá»‡u. |

#### Quy trÃ¬nh Ä‘á» xuáº¥t:

1.  **Dá»¯ liá»‡u cÃ³ outliers khÃ´ng?**

    -   **KhÃ´ng**: DÃ¹ng **MSE**.

    -   **CÃ³**: DÃ¹ng **Huber Loss** (hoáº·c **MAE**).

2.  **CÃ¡c Ä‘áº·c trÆ°ng cÃ³ thang Ä‘o khÃ¡c nhau khÃ´ng?**

    -   LuÃ´n luÃ´n thá»±c hiá»‡n **Chuáº©n hÃ³a dá»¯ liá»‡u**.

3.  **Lo ngáº¡i vá» overfitting?**

    -   Sá»­ dá»¥ng **Regularization** (L1 hoáº·c L2).

Viá»‡c lá»±a chá»n Ä‘Ãºng hÃ m máº¥t mÃ¡t lÃ  má»™t nghá»‡ thuáº­t, Ä‘Ã²i há»i sá»± am hiá»ƒu vá» dá»¯ liá»‡u vÃ  má»¥c tiÃªu bÃ i toÃ¡n Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c vÃ  Ä‘Ã¡ng tin cáº­y.

---

## **6. XAI(LIME)**

### **6.1. TÃ³m táº¯t: Explainable AI (XAI) vÃ  LIME**

#### **Interpretability (TÃ­nh Diá»…n giáº£i) lÃ  gÃ¬ vÃ  Táº¡i sao chÃºng ta cáº§n nÃ³?**

**TÃ­nh diá»…n giáº£i (Interpretability)** lÃ  má»©c Ä‘á»™ mÃ  con ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c nguyÃªn nhÃ¢n dáº«n Ä‘áº¿n má»™t quyáº¿t Ä‘á»‹nh do mÃ´ hÃ¬nh AI Ä‘Æ°a ra. Trong khi cÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n nhÆ° CÃ¢y Quyáº¿t Ä‘á»‹nh (Decision Trees) vá»‘n dÄ© dá»… hiá»ƒu, cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n nhÆ° Máº¡ng NÆ¡-ron SÃ¢u (Deep Neural Networks) láº¡i hoáº¡t Ä‘á»™ng nhÆ° nhá»¯ng **"há»™p Ä‘en" (black boxes)**, khiáº¿n cho quÃ¡ trÃ¬nh ra quyáº¿t Ä‘á»‹nh cá»§a chÃºng trá»Ÿ nÃªn má» má»‹t.

Sá»± thiáº¿u minh báº¡ch nÃ y, Ä‘Æ°á»£c gá»i lÃ  **Váº¥n Ä‘á» Há»™p Ä‘en AI (AI Black Box Problem)**, táº¡o ra má»™t sá»‘ thÃ¡ch thá»©c:

* **Sá»± tin cáº­y vÃ  TrÃ¡ch nhiá»‡m giáº£i trÃ¬nh:** Ráº¥t khÃ³ Ä‘á»ƒ tin tÆ°á»Ÿng vÃ o má»™t quyáº¿t Ä‘á»‹nh náº¿u báº¡n khÃ´ng thá»ƒ hiá»ƒu Ä‘Æ°á»£c lÃ½ do Ä‘áº±ng sau nÃ³.

* **Gá»¡ lá»—i (Debugging):** Náº¿u khÃ´ng hiá»ƒu Ä‘Æ°á»£c logic cá»§a mÃ´ hÃ¬nh, viá»‡c xÃ¡c Ä‘á»‹nh vÃ  sá»­a lá»—i sáº½ ráº¥t khÃ³ khÄƒn.

* **Sá»± cÃ´ng báº±ng vÃ  TuÃ¢n thá»§:** Viá»‡c giáº£i thÃ­ch lÃ  ráº¥t quan trá»ng Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c há»‡ thá»‘ng AI Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cÃ´ng báº±ng vÃ  tuÃ¢n thá»§ cÃ¡c quy Ä‘á»‹nh.

**AI cÃ³ thá»ƒ giáº£i thÃ­ch (Explainable AI - XAI)** nháº±m giáº£i quyáº¿t nhá»¯ng váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch cung cáº¥p cÃ¡c ká»¹ thuáº­t Ä‘á»ƒ hiá»ƒu vÃ  tin tÆ°á»Ÿng vÃ o káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y.

---

### **6.2. Sá»± phÃ¡t triá»ƒn vÃ  PhÃ¢n loáº¡i cá»§a XAI** ğŸ“ˆ

LÄ©nh vá»±c XAI Ä‘Ã£ phÃ¡t triá»ƒn Ä‘Ã¡ng ká»ƒ theo thá»i gian:

1.  **Há»‡ thá»‘ng KÃ½ hiá»‡u SÆ¡ khai (1950s-1980s):** CÃ¡c há»‡ thá»‘ng AI ban Ä‘áº§u nhÆ° há»‡ chuyÃªn gia (expert systems) Ä‘Æ°á»£c thiáº¿t káº¿ dá»±a trÃªn luáº­t vÃ  cÃ³ tÃ­nh minh báº¡ch.

2.  **ThÃ¡ch thá»©c Há»™p Ä‘en (1980s-Hiá»‡n táº¡i):** Sá»± trá»—i dáº­y cá»§a máº¡ng nÆ¡-ron Ä‘Ã£ giá»›i thiá»‡u cÃ¡c mÃ´ hÃ¬nh máº¡nh máº½ nhÆ°ng khÃ´ng minh báº¡ch, táº¡o ra nhu cáº§u vá» cÃ¡c cÃ´ng cá»¥ diá»…n giáº£i má»›i.

3.  **Sá»± ra Ä‘á»i cá»§a cÃ¡c PhÆ°Æ¡ng phÃ¡p XAI (2016-Hiá»‡n táº¡i):** CÃ¡c ká»¹ thuáº­t nhÆ° LIME vÃ  SHAP Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ cung cáº¥p cÃ¡i nhÃ¬n sÃ¢u sáº¯c vá» cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p.

CÃ¡c ká»¹ thuáº­t XAI cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i rá»™ng rÃ£i nhÆ° sau:

* **Ante-hoc vÃ  Post-hoc:** PhÆ°Æ¡ng phÃ¡p Ante-hoc dÃ nh cho cÃ¡c mÃ´ hÃ¬nh cÃ³ báº£n cháº¥t dá»… diá»…n giáº£i (vÃ­ dá»¥: CÃ¢y Quyáº¿t Ä‘á»‹nh). PhÆ°Æ¡ng phÃ¡p Post-hoc Ä‘Æ°á»£c Ã¡p dá»¥ng *sau khi* má»™t mÃ´ hÃ¬nh phá»©c táº¡p Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ giáº£i thÃ­ch cÃ¡c dá»± Ä‘oÃ¡n cá»§a nÃ³.

* **Phá»¥ thuá»™c vÃ o MÃ´ hÃ¬nh (Model-Specific) vÃ  Báº¥t biáº¿n vá»›i MÃ´ hÃ¬nh (Model-Agnostic):** CÃ¡c ká»¹ thuáº­t phá»¥ thuá»™c vÃ o mÃ´ hÃ¬nh chá»‰ gáº¯n vá»›i má»™t kiáº¿n trÃºc mÃ´ hÃ¬nh cá»¥ thá»ƒ (vÃ­ dá»¥: Grad-CAM cho CNN). CÃ¡c phÆ°Æ¡ng phÃ¡p báº¥t biáº¿n vá»›i mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng trÃªn báº¥t ká»³ mÃ´ hÃ¬nh nÃ o. LIME lÃ  má»™t ká»¹ thuáº­t post-hoc vÃ  báº¥t biáº¿n vá»›i mÃ´ hÃ¬nh.

---

### **6.3. LIME: Giáº£i thÃ­ch Cá»¥c bá»™ Diá»…n giáº£i Ä‘Æ°á»£c vÃ  Báº¥t biáº¿n vá»›i MÃ´ hÃ¬nh**

**LIME** lÃ  má»™t thuáº­t toÃ¡n XAI phá»• biáº¿n giÃºp giáº£i thÃ­ch dá»± Ä‘oÃ¡n cá»§a báº¥t ká»³ mÃ´ hÃ¬nh há»™p Ä‘en phá»©c táº¡p nÃ o báº±ng cÃ¡ch xáº¥p xá»‰ nÃ³ vá»›i má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÆ¡n, cÃ³ thá»ƒ diá»…n giáº£i Ä‘Æ°á»£c (nhÆ° há»“i quy tuyáº¿n tÃ­nh) trong pháº¡m vi cá»¥c bá»™ cá»§a dá»± Ä‘oÃ¡n cáº§n Ä‘Æ°á»£c giáº£i thÃ­ch.

Ã tÆ°á»Ÿng cá»‘t lÃµi lÃ  tÃ¬m hiá»ƒu hÃ nh vi cá»§a má»™t mÃ´ hÃ¬nh phá»©c táº¡p Ä‘á»‘i vá»›i má»™t **dá»± Ä‘oÃ¡n Ä‘Æ¡n láº»** báº±ng cÃ¡ch lÃ m nhiá»…u Ä‘áº§u vÃ o vÃ  xem cÃ¡c dá»± Ä‘oÃ¡n thay Ä‘á»•i nhÆ° tháº¿ nÃ o.

#### **CÃ¡ch LIME hoáº¡t Ä‘á»™ng: VÃ­ dá»¥ vá»›i HÃ¬nh áº£nh**

Giáº£ sá»­ má»™t mÃ´ hÃ¬nh phá»©c táº¡p dá»± Ä‘oÃ¡n má»™t hÃ¬nh áº£nh cÃ³ chá»©a "con áº¿ch cÃ³ Ä‘uÃ´i". LIME giáº£i thÃ­ch dá»± Ä‘oÃ¡n nÃ y qua cÃ¡c bÆ°á»›c sau:

1.  **PhÃ¢n Ä‘oáº¡n (Segmentation):** HÃ¬nh áº£nh gá»‘c Ä‘Æ°á»£c chia thÃ nh cÃ¡c "siÃªu Ä‘iá»ƒm áº£nh" (superpixels), lÃ  nhá»¯ng máº£ng liá»n ká» gá»“m cÃ¡c pixel tÆ°Æ¡ng tá»± nhau. Nhá»¯ng siÃªu Ä‘iá»ƒm áº£nh nÃ y trá»Ÿ thÃ nh cÃ¡c Ä‘áº·c trÆ°ng cÃ³ thá»ƒ diá»…n giáº£i Ä‘Æ°á»£c.

    ```python
    import skimage.segmentation
    
    # Táº¡o cÃ¡c siÃªu Ä‘iá»ƒm áº£nh cho áº£nh Ä‘áº§u vÃ o
    def generate_superpixels(image):
        """Táº¡o cÃ¡c siÃªu Ä‘iá»ƒm áº£nh báº±ng thuáº­t toÃ¡n quickshift.""" #
        superpixels = skimage.segmentation.quickshift(image, kernel_size=21, max_dist=200, ratio=0.2) #
        return superpixels #
    
    superpixels = generate_superpixels(my_image) #
    ```

2.  **GÃ¢y nhiá»…u (Perturbation):** Táº¡o má»™t bá»™ dá»¯ liá»‡u gá»“m cÃ¡c hÃ¬nh áº£nh má»›i báº±ng cÃ¡ch áº©n hoáº·c hiá»‡n ngáº«u nhiÃªn cÃ¡c káº¿t há»£p khÃ¡c nhau cá»§a cÃ¡c siÃªu Ä‘iá»ƒm áº£nh.

3.  **Dá»± Ä‘oÃ¡n (Prediction):** Sá»­ dá»¥ng mÃ´ hÃ¬nh phá»©c táº¡p ban Ä‘áº§u Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cÃ³ "con áº¿ch cÃ³ Ä‘uÃ´i" cho má»—i hÃ¬nh áº£nh Ä‘Ã£ bá»‹ lÃ m nhiá»…u.

4.  **GÃ¡n trá»ng sá»‘ (Weighting):** GÃ¡n trá»ng sá»‘ cho má»—i áº£nh bá»‹ nhiá»…u dá»±a trÃªn sá»± gáº§n gÅ©i cá»§a nÃ³ vá»›i áº£nh gá»‘c. CÃ¡c máº«u tÆ°Æ¡ng tá»± hÆ¡n (tá»©c lÃ  cÃ³ Ã­t siÃªu Ä‘iá»ƒm áº£nh bá»‹ áº©n hÆ¡n) sáº½ Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ cao hÆ¡n. Äiá»u nÃ y thÆ°á»ng Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng má»™t thÆ°á»›c Ä‘o khoáº£ng cÃ¡ch nhÆ° khoáº£ng cÃ¡ch cosine vÃ  hÃ m nhÃ¢n (kernel function) mÅ©.

    ```python
    import numpy as np
    import sklearn.metrics
    
    # HÃ m tÃ­nh toÃ¡n trá»ng sá»‘ dá»±a trÃªn khoáº£ng cÃ¡ch
    def compute_distances_and_weights(perturbations, num_superpixels, kernel_width=8.25): #
        original_image = np.ones(num_superpixels)[np.newaxis,:] #
        distances = sklearn.metrics.pairwise_distances(perturbations, original_image, metric='cosine').ravel() #
        weights = np.sqrt(np.exp(-(distances**2) / kernel_width**2)) #
        return weights
    ```

5.  **Huáº¥n luyá»‡n MÃ´ hÃ¬nh Diá»…n giáº£i Ä‘Æ°á»£c:** Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh cÃ³ trá»ng sá»‘ Ä‘Æ¡n giáº£n trÃªn bá»™ dá»¯ liá»‡u gá»“m cÃ¡c áº£nh bá»‹ nhiá»…u vÃ  cÃ¡c dá»± Ä‘oÃ¡n tÆ°Æ¡ng á»©ng cá»§a chÃºng. CÃ¡c Ä‘áº·c trÆ°ng lÃ  biá»ƒu diá»…n nhá»‹ phÃ¢n cá»§a cÃ¡c siÃªu Ä‘iá»ƒm áº£nh vÃ  má»¥c tiÃªu lÃ  dá»± Ä‘oÃ¡n tá»« mÃ´ hÃ¬nh phá»©c táº¡p.

    ```python
    from sklearn.linear_model import LinearRegression
    
    # Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n
    simpler_model = LinearRegression() #
    simpler_model.fit(X=perturbations, y=probabilities[:, 0, class_to_explain], sample_weight=weights) #
    ```

6.  **Giáº£i thÃ­ch:** CÃ¡c há»‡ sá»‘ cá»§a mÃ´ hÃ¬nh tuyáº¿n tÃ­nh Ä‘Ã£ huáº¥n luyá»‡n cho tháº¥y táº§m quan trá»ng cá»§a má»—i siÃªu Ä‘iá»ƒm áº£nh. Má»™t há»‡ sá»‘ dÆ°Æ¡ng cao cÃ³ nghÄ©a lÃ  siÃªu Ä‘iá»ƒm áº£nh Ä‘Ã³ Ä‘Ã£ Ä‘Ã³ng gÃ³p máº¡nh máº½ vÃ o dá»± Ä‘oÃ¡n "con áº¿ch cÃ³ Ä‘uÃ´i".

#### **LIME cho VÄƒn báº£n**

Quy trÃ¬nh tÆ°Æ¡ng tá»± cÅ©ng Ã¡p dá»¥ng cho dá»¯ liá»‡u vÄƒn báº£n:

1.  **Máº«u (Instance):** Chá»n cÃ¢u cáº§n giáº£i thÃ­ch (vÃ­ dá»¥: "Bá»™ phim tháº­t sá»± tuyá»‡t vá»i vÃ  diá»…n xuáº¥t tháº­t xuáº¥t sáº¯c.").

2.  **GÃ¢y nhiá»…u:** Táº¡o ra cÃ¡c phiÃªn báº£n má»›i cá»§a cÃ¢u báº±ng cÃ¡ch loáº¡i bá» ngáº«u nhiÃªn cÃ¡c tá»«.

3.  **Dá»± Ä‘oÃ¡n:** Láº¥y dá»± Ä‘oÃ¡n vá» sáº¯c thÃ¡i (vÃ­ dá»¥: xÃ¡c suáº¥t lÃ  "TÃ­ch cá»±c") cho má»—i cÃ¢u Ä‘Ã£ bá»‹ lÃ m nhiá»…u tá»« mÃ´ hÃ¬nh há»™p Ä‘en.

4.  **GÃ¡n trá»ng sá»‘:** GÃ¡n trá»ng sá»‘ cao hÆ¡n cho cÃ¡c máº«u tÆ°Æ¡ng tá»± hÆ¡n vá»›i vÄƒn báº£n gá»‘c (cÃ ng Ã­t tá»« bá»‹ loáº¡i bá» thÃ¬ cÃ ng tÆ°Æ¡ng tá»±).

5.  **Huáº¥n luyá»‡n MÃ´ hÃ¬nh:** Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh tuyáº¿n tÃ­nh cÃ³ trá»ng sá»‘, trong Ä‘Ã³ cÃ¡c Ä‘áº·c trÆ°ng lÃ  cÃ¡c chá»‰ sá»‘ nhá»‹ phÃ¢n cho sá»± hiá»‡n diá»‡n cá»§a má»—i tá»«.

6.  **Giáº£i thÃ­ch:** CÃ¡c há»‡ sá»‘ cá»§a mÃ´ hÃ¬nh cho tháº¥y nhá»¯ng tá»« nÃ o ("tuyá»‡t vá»i", "xuáº¥t sáº¯c") cÃ³ áº£nh hÆ°á»Ÿng tÃ­ch cá»±c nháº¥t Ä‘áº¿n dá»± Ä‘oÃ¡n.

---

### **6.4. Æ¯u Ä‘iá»ƒm vÃ  NhÆ°á»£c Ä‘iá»ƒm cá»§a LIME**

**Æ¯u Ä‘iá»ƒm:** ğŸ‘
* **Dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai**.
* **Báº¥t biáº¿n vá»›i MÃ´ hÃ¬nh (Model-Agnostic):** Hoáº¡t Ä‘á»™ng vá»›i má»i loáº¡i mÃ´ hÃ¬nh.
* **Cung cáº¥p cÃ¡c giáº£i thÃ­ch cá»¥c bá»™** trá»±c quan cho cÃ¡c dá»± Ä‘oÃ¡n riÃªng láº».

**NhÆ°á»£c Ä‘iá»ƒm:** ğŸ‘
* **Thiáº¿u á»•n Ä‘á»‹nh:** CÃ¡c giáº£i thÃ­ch cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh.
* **Tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n:** YÃªu cáº§u thá»±c hiá»‡n nhiá»u dá»± Ä‘oÃ¡n trÃªn cÃ¡c máº«u bá»‹ nhiá»…u.
* Äá»‹nh nghÄ©a vá» "cá»¥c bá»™"